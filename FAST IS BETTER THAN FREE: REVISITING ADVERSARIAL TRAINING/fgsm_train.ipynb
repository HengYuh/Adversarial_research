{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import some neccesary module\n",
    "import copy\n",
    "import logging\n",
    "import os\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from apex import amp\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preact_resnet import PreActResNet18\n",
    "from utils import (upper_limit, lower_limit, std, clamp, get_loaders,\n",
    "    attack_pgd, evaluate_pgd, evaluate_standard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter Setting\n",
    "out_dir = \"/home/ubuntu/zhc/adversarial_attack/fast_adversarial/CIFAR10/output\"\n",
    "seed = 7\n",
    "data_dir = '../../cifar-data'\n",
    "batch_size = 128\n",
    "epsilon = 8\n",
    "alpha = 10\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "lr_max = 0.2\n",
    "momentum = 0.9\n",
    "weight_decay = 5e-4\n",
    "opt_level = 'O2'\n",
    "loss_scale = '1.0'\n",
    "delta_init = 'random'\n",
    "epochs = 15\n",
    "lr_schedule = 'cyclic'\n",
    "lr_min = 0.\n",
    "early_stop = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if not exists the out_dir, we create it.\n",
    "if not os.path.exists(out_dir):\n",
    "    os.mkdir(out_dir)\n",
    "\n",
    "# and we create a logfile path\n",
    "logfile = os.path.join(out_dir, 'output.log')\n",
    "\n",
    "# if logfile exists, we remove it \n",
    "if os.path.exists(logfile):\n",
    "    os.remove(logfile)\n",
    "\n",
    "logging.basicConfig(\n",
    "    format='[%(asctime)s] - %(message)s',\n",
    "    datefmt='%Y/%m/%d %H:%M:%S',\n",
    "    level=logging.INFO,\n",
    "    filename=os.path.join(out_dir, 'output.log'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set random seed \n",
    "\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# data preprocess and load \n",
    "train_loader, test_loader = get_loaders(data_dir, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = (epsilon / 255.) / std\n",
    "alpha = (alpha / 255.) / std\n",
    "pgd_alpha = (2 / 255.) / std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PreActResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): PreActBlock(\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    )\n",
       "    (1): PreActBlock(\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): PreActBlock(\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (shortcut): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (1): PreActBlock(\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): PreActBlock(\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (shortcut): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (1): PreActBlock(\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): PreActBlock(\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (shortcut): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (1): PreActBlock(\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    )\n",
       "  )\n",
       "  (bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (linear): Linear(in_features=512, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = PreActResNet18().to(device)\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizor: SGD\n",
    "opt = torch.optim.SGD(model.parameters(), lr = lr_max, momentum=momentum, weight_decay=weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "amp_args = dict(opt_level=opt_level, loss_scale=loss_scale, verbosity=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "if opt_level == 'O2':\n",
    "    amp_args['master_weights'] = True\n",
    "model, opt = amp.initialize(model, opt, **amp_args)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# criterion function: CrossEntropy\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "if delta_init == 'previous':\n",
    "    delta = torch.zeros(batch_size, 3, 32, 32).cuda()\n",
    "\n",
    "lr_steps = epochs * len(train_loader)\n",
    "\n",
    "if lr_schedule == 'cyclic':\n",
    "    scheduler = torch.optim.lr_scheduler.CyclicLR(opt, base_lr=lr_min, max_lr=lr_max,\n",
    "        step_size_up=lr_steps / 2, step_size_down=lr_steps / 2)\n",
    "elif lr_schedule == 'multistep':\n",
    "    scheduler = torch.optim.lr_scheduler.MultiStepLR(opt, milestones=[lr_steps / 2, lr_steps * 3 / 4], gamma=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "prev_robust_acc = 0.\n",
    "start_train_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/15 [00:00<?, ?it/s]/home/ubuntu/anaconda3/envs/adver_attack/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:1283: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  \"please use `get_last_lr()`.\", UserWarning)\n",
      "  7%|▋         | 1/15 [00:18<04:25, 18.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "%d \t %.1f \t \t %.4f \t %.4f \t %.4f 0 18.965568780899048 0.02666666666666666 2.1306632760620117 0.19348\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 2/15 [00:36<03:57, 18.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "%d \t %.1f \t \t %.4f \t %.4f \t %.4f 1 17.83906054496765 0.05333333333333332 1.918559842529297 0.27552\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 3/15 [00:54<03:37, 18.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "%d \t %.1f \t \t %.4f \t %.4f \t %.4f 2 17.83459162712097 0.07999999999999999 1.825787401046753 0.30774\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 4/15 [01:12<03:17, 17.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "%d \t %.1f \t \t %.4f \t %.4f \t %.4f 3 17.802282333374023 0.10666666666666665 1.7358898900604247 0.33736\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 5/15 [01:30<02:59, 17.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "%d \t %.1f \t \t %.4f \t %.4f \t %.4f 4 17.799811601638794 0.1333333333333333 1.6520530545043945 0.36958\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 6/15 [01:48<02:40, 17.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "%d \t %.1f \t \t %.4f \t %.4f \t %.4f 5 17.759487867355347 0.15999999999999998 1.5971660720825196 0.3933\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 7/15 [02:05<02:22, 17.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "%d \t %.1f \t \t %.4f \t %.4f \t %.4f 6 17.76990270614624 0.18666666666666673 1.5517124658584596 0.4078\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 8/15 [02:23<02:04, 17.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "%d \t %.1f \t \t %.4f \t %.4f \t %.4f 7 17.7959885597229 0.18666666666666673 1.5124848135375977 0.4247\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 9/15 [02:41<01:47, 17.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "%d \t %.1f \t \t %.4f \t %.4f \t %.4f 8 17.868568181991577 0.15999999999999998 1.4708707464599609 0.43812\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 10/15 [02:59<01:29, 17.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "%d \t %.1f \t \t %.4f \t %.4f \t %.4f 9 17.86358618736267 0.1333333333333334 1.4272908445739747 0.45386\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 11/15 [03:17<01:11, 17.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "%d \t %.1f \t \t %.4f \t %.4f \t %.4f 10 17.78630518913269 0.10666666666666665 1.3926101739501953 0.46768\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 12/15 [03:34<00:53, 17.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "%d \t %.1f \t \t %.4f \t %.4f \t %.4f 11 17.784669160842896 0.07999999999999999 1.3521190425491334 0.48286\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 13/15 [03:52<00:35, 17.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "%d \t %.1f \t \t %.4f \t %.4f \t %.4f 12 17.825743198394775 0.05333333333333332 1.296216632080078 0.50012\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 14/15 [04:10<00:17, 17.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "%d \t %.1f \t \t %.4f \t %.4f \t %.4f 13 17.831653594970703 0.02666666666666666 1.230075249633789 0.5264\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [04:28<00:00, 17.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "%d \t %.1f \t \t %.4f \t %.4f \t %.4f 14 17.78027582168579 0.0 1.129176103439331 0.55982\n",
      "4.65578502813975\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss \t Test Acc \t PGD Loss \t PGD Acc\n",
      "%.4f \t \t %.4f \t %.4f \t %.4f 0.664918485736847 0.7917 1.0259269359588623 0.445\n"
     ]
    }
   ],
   "source": [
    "total_batches = len(train_loader)\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    start_epoch_time = time.time()\n",
    "    train_loss = 0\n",
    "    train_acc = 0\n",
    "    train_n = 0\n",
    "    for i, (X, y) in enumerate(train_loader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        if i == 0:\n",
    "            first_batch = (X, y)\n",
    "        if delta_init != 'previous':\n",
    "            delta = torch.zeros_like(X).cuda()\n",
    "        if delta_init == 'random':\n",
    "            for j in range(len(epsilon)):\n",
    "                delta[:, j, :, :].uniform_(-epsilon[j][0][0].item(), epsilon[j][0][0].item())\n",
    "            delta.data = clamp(delta, lower_limit - X, upper_limit - X)\n",
    "            \n",
    "        delta.requires_grad = True\n",
    "        output = model(X + delta[:X.size(0)])\n",
    "        loss = F.cross_entropy(output, y)\n",
    "        with amp.scale_loss(loss, opt) as scaled_loss:\n",
    "            scaled_loss.backward()\n",
    "        grad = delta.grad.detach()\n",
    "        delta.data = clamp(delta + alpha * torch.sign(grad), -epsilon, epsilon)\n",
    "        delta.data[:X.size(0)] = clamp(delta[:X.size(0)], lower_limit - X, upper_limit - X)\n",
    "        delta = delta.detach()\n",
    "        output = model(X + delta[:X.size(0)])\n",
    "        loss = criterion(output, y)\n",
    "        opt.zero_grad()\n",
    "        with amp.scale_loss(loss, opt) as scaled_loss:\n",
    "            scaled_loss.backward()\n",
    "        opt.step()\n",
    "        train_loss += loss.item() * y.size(0)\n",
    "        train_acc += (output.max(1)[1] == y).sum().item()\n",
    "        train_n += y.size(0)\n",
    "        scheduler.step()\n",
    "    if early_stop:\n",
    "        # Check current PGD robustness of model using random minibatch\n",
    "        X, y = first_batch\n",
    "        pgd_delta = attack_pgd(model, X, y, epsilon, pgd_alpha, 5, 1, opt)\n",
    "        with torch.no_grad():\n",
    "            output = model(clamp(X + pgd_delta[:X.size(0)], lower_limit, upper_limit))\n",
    "        robust_acc = (output.max(1)[1] == y).sum().item() / y.size(0)\n",
    "        if robust_acc - prev_robust_acc < -0.2:\n",
    "            break\n",
    "        prev_robust_acc = robust_acc\n",
    "        best_state_dict = copy.deepcopy(model.state_dict())\n",
    "    epoch_time = time.time()\n",
    "    lr = scheduler.get_lr()[0]\n",
    "    print('%d \\t %.1f \\t \\t %.4f \\t %.4f \\t %.4f',\n",
    "            epoch, epoch_time - start_epoch_time, lr, train_loss/train_n, train_acc/train_n)\n",
    "\n",
    "train_time = time.time()\n",
    "\n",
    "if not early_stop:\n",
    "    best_state_dict = model.state_dict()\n",
    "torch.save(best_state_dict, os.path.join(out_dir, 'model.pth'))\n",
    "print((train_time - start_train_time)/60)\n",
    "\n",
    "# Evaluation\n",
    "model_test = PreActResNet18().to(device)\n",
    "model_test.load_state_dict(best_state_dict)\n",
    "model_test.float()\n",
    "model_test.eval()\n",
    "\n",
    "pgd_loss, pgd_acc = evaluate_pgd(test_loader, model_test, 50, 10)\n",
    "test_loss, test_acc = evaluate_standard(test_loader, model_test)\n",
    "\n",
    "print('Test Loss \\t Test Acc \\t PGD Loss \\t PGD Acc')\n",
    "print('%.4f \\t \\t %.4f \\t %.4f \\t %.4f', test_loss, test_acc, pgd_loss, pgd_acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "adver_attack",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
